{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Baseline for Training and Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DoTLMViz.transformers import Embedding, PosEmbedding, LayerNorm, Attention, Unembedding, Config\n",
    "from DoTLMViz.datamodules import Piles10k\n",
    "\n",
    "from jaxtyping import Float\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating a baseline for training and evaluating out custom attention only transformer language models in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Configuration*\n",
    "\n",
    "To start training our model, we will first need to assemble it. We have developed all the modules that will make the 1L-attn-only model to take a `Config` which contains various configuration for the model:\n",
    "\n",
    "1. `d_model` - the size of the embedding\n",
    "2. `layer_norm_eps` - a small constant that is added on the variance during normalization.\n",
    "3. `d_vocab` - the size of the vocabulary\n",
    "4. `init_range` - the range of values in the paramters\n",
    "5. `n_ctx` - the context length\n",
    "6. `d_head` - the dimension of each attention head\n",
    "7. `d_mlp` - the dimension of mlp\n",
    "8. `n_heads` - the number of attention heads\n",
    "9. `n_layers` - the number of transformer blocks\n",
    "\n",
    "For a 1L-attn-only model, we won't need the mlp and the number of layers will be only one, so the configuration will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerAttnOnlyConfig(Config):\n",
    "    d_model: int = 768\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 128\n",
    "    d_head: int = 64\n",
    "    n_heads: int = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our configuration, we can assemble our 1L-attn-only model using the modules - `Embedding`, `PosEmbedding`, `LayerNorm`, `Attention`, `Unembedding` - that we have already developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneLayerAttnOnlyModel(nn.Module):\n",
    "    \"\"\"One Layer Attention Only Transformer Language Model.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.embed = Embedding(config)\n",
    "        self.pos_embed = PosEmbedding(config)\n",
    "        self.ln1 = LayerNorm(config)\n",
    "        self.attn = Attention(config)\n",
    "        self.ln2 = LayerNorm(config)\n",
    "        self.unembed = Unembedding(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The forward method takes an input `x` on which the One Layer Attention Only\n",
    "        Transformer Model operates to produce logits.\n",
    "        \"\"\"\n",
    "        x = self.embed(x) + self.pos_embed(x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.unembed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our model, and we have also implemented how the input passes through it to form the logits in the `forward` method of the model. But, *what kind of data is our model operating on?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneLayerAttnOnlyModel(config=OneLayerAttnOnlyConfig).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Piles-10k dataset to train our model, which we have made available through the Piles10k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = Piles10k(batch_size=32, max_length=OneLayerAttnOnlyConfig.n_ctx)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = next(iter(datamodule.train_dataloader()))\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each batch in our dataloaders consist of 32 sequences, each of length 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Loop (For a single batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, we need to do the following:\n",
    "\n",
    "1. Perform a forward pass to obtain logits.\n",
    "2. Use the logits to calculate the loss.\n",
    "3. Perform a backward pass to backpropagate the loss.\n",
    "4. Use an optimizer to optimize the parameters.\n",
    "\n",
    "The above 4 steps are repeated for a certain number of steps/epochs or until some other condition is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instantiate a model first,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneLayerAttnOnlyModel(config=OneLayerAttnOnlyConfig).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a Single Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Perform a forward pass to obtain the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform an example of forward pass, let us obtain a sample from the dataloader first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(datamodule.train_dataloader()))\n",
    "sample = sample.to(\"cuda\")\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the forward pass can be performed by simply passing the data from the model. As a result of the forward pass, we will obtain logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Use the logits to calculate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss we will be using to update our model will be the Negative Loglikelihood Loss. The Negative Loglikelihood Loss can be computed by using the logits as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 50257])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Compute the log likelihood for all tokens in the vocabulary at each position in the sequence\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 127])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. From the computed log probabilities, find the predicted log probabilities for the next tokens for each position in the sequence.\n",
    "pred_log_probs = torch.gather(log_probs[:, :-1], -1, sample[:, 1:, None])[..., 0]\n",
    "pred_log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9717, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Compute the mean of the predicted log probability for the next token for each position in the sequence.\n",
    "-pred_log_probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping all of this in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(\n",
    "    logits: Float[torch.Tensor, \"batch seq d_vocab\"], tokens: Float[torch.Tensor, \"batch seq\"]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns cross entropy loss for the given logits and tokens.\n",
    "    \"\"\"\n",
    "    log_probs: Float[torch.Tensor, \"batch seq d_vocab\"] = F.log_softmax(logits, dim=-1)\n",
    "    pred_log_probs: Float[torch.Tensor, \"batch seq\"] = torch.gather(log_probs[:, :-1], -1, tokens[:, 1:, None])[..., 0]\n",
    "    return -pred_log_probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Perform a backward pass to backpropagate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss can be backpropagated through the model by simply calling the `backward` method on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cross_entropy_loss(logits, sample)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, we will be backpropagating the loss for each batch inside an epoch. To ensure that the gradient from one batch doesn't affect another batch, we should zero out all the grads. We do this by using an optimizer.\n",
    "\n",
    ">**Aside: What about other Optimizers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we should zero out the grads before backpropagating it. Thus, we have the following so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(sample)\n",
    "loss = cross_entropy_loss(logits, sample)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use an optimizer to optimize the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters can be optimized by simply calling the `step` method of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the training loop performs the following for each batch inside each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(sample)\n",
    "loss = cross_entropy_loss(logits, sample)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through all Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate through each batch ins the train dataloader and execute the code to train the model on a single batch for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in datamodule.train_dataloader():\n",
    "    logits = model(sample)\n",
    "    loss = cross_entropy_loss(logits, sample)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we can use accuracy as the metric.\n",
    "\n",
    ">**Aside: Metric**\n",
    ">\n",
    ">Is accuracy a good metric for the task of language generation? If not, then which metric should we use, and can you justify its use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct, total_sample = 0, 0\n",
    "    for sample in datamodule.val_dataloader():\n",
    "        logits = model(sample)\n",
    "        predicted_tokens = logits[:, :-1].argmax(dim=-1)\n",
    "        total_correct += (predicted_tokens == sample[:, 1:]).sum().item()\n",
    "        total_sample += sample.size(0) * (sample.size(1) - 1)\n",
    "\n",
    "    accuracy = total_correct / total_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Through all Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can iterate through each epoch and perform the above steps by logging the informations as:\n",
    "\n",
    ">**Aside: Number of batches** \n",
    ">\n",
    ">It can take forever to run this for the entire batches in the dataloader. Could you do it so that the training is only performed for a specific number of batches per epoch? **Will training the model like this be justified?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    losses, accuracies = [], []\n",
    "\n",
    "    # training part\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for sample in datamodule.train_dataloader():\n",
    "        logits = model(sample)\n",
    "        loss = cross_entropy_loss(logits, sample)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    mean_loss = total_loss / len(datamodule.train_dataloader())  # per epoch\n",
    "    losses.append(mean_loss)\n",
    "\n",
    "    # evaluation part\n",
    "\n",
    "    model.eval()\n",
    "    total_correct, total_sample = 0, 0\n",
    "\n",
    "    for sample in datamodule.val_dataloader():\n",
    "        logits = model(sample)\n",
    "        predicted_tokens = logits[:, :-1].argmax(dim=-1)\n",
    "        total_correct += (predicted_tokens == sample[:, 1:]).sum().item()\n",
    "        total_sample += sample.size(0) * (sample.size(1) - 1)\n",
    "\n",
    "    accuracy = total_correct / total_sample\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"[{epoch + 1}/{epochs}]\\tloss: {mean_loss}\\tacc: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
